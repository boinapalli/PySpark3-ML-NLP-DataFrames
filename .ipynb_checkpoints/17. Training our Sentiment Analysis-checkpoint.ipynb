{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"ModelTraining\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "schema = \"polarity FLOAT, id LONG, date_time TIMESTAMP, query STRING, user STRING, text STRING\"\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "\n",
    "\n",
    "IN_PATH = \"datasets/sentiment-140-training-data/CLEAN\"\n",
    "OUT_PATH = \"datasets/sentiment-140-training-data/MODEL\"\n",
    "\n",
    "spark_reader = spark.read.schema(schema)\n",
    "\n",
    "\n",
    "df_clean = spark_reader.parquet(IN_PATH)\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    # Remove all numbers\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \"[^a-zA-Z']\", \" \"))\n",
    "    # Remove all double/multiple spaces\n",
    "    .withColumn(\"text\", f.regexp_replace(f.col(\"text\"), \" +\", \" \"))\n",
    "    # Remove leading and trailing whitespaces\n",
    "    .withColumn(\"text\", f.trim(f.col(\"text\")))\n",
    "    # Ensure we don't end up with empty rows\n",
    "    .filter(\"text != ''\")\n",
    ")\n",
    "\n",
    "data = df_clean.select(\"text\", \"polarity\").coalesce(3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, validation_data, test_data) = data.randomSplit([0.98, 0.01, 0.01], seed=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import (\n",
    "    StopWordsRemover,\n",
    "    Tokenizer,\n",
    "    HashingTF,\n",
    "    IDF\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Tokenizer converts input string to lowercase and then splits it by white spaces.\n",
    "# https://spark.apache.org/docs/3.0.1/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer\n",
    "# Params:\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"text\",\n",
    "    outputCol=\"words1\"\n",
    ")\n",
    "\n",
    "# A feature transformer that filters out stop words from input.\n",
    "# https://spark.apache.org/docs/3.0.1/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\n",
    "# Params:\n",
    "stopword_remover = StopWordsRemover(\n",
    "    inputCol=\"words1\",\n",
    "    outputCol=\"words2\",\n",
    "    stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    ")\n",
    "\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick\n",
    "# https://spark.apache.org/docs/3.0.1/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\n",
    "# Params:\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol=\"words2\",\n",
    "    outputCol=\"term_frequency\"\n",
    ")\n",
    "\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents\n",
    "# https://spark.apache.org/docs/3.0.1/api/python/pyspark.ml.html#pyspark.ml.feature.IDF\n",
    "# Params:\n",
    "idf = IDF(\n",
    "    inputCol=\"term_frequency\",\n",
    "    outputCol=\"features\",\n",
    "    minDocFreq=5\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"polarity\")\n",
    "\n",
    "semantic_analysis_pipeline = Pipeline(\n",
    "        stages = [\n",
    "            tokenizer,\n",
    "            stopword_remover,\n",
    "            hashing_tf,\n",
    "            idf,\n",
    "            lr\n",
    "        ]\n",
    ")\n",
    "\n",
    "#semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                   '|     4.0|                 [']|                 [']|(262144,[186171],...|(262144,[186171],...|[8.07978709803161...|[0.46818513567162...|       4.0|\n",
      "|         ' ' ' ' ' '|     4.0|  [', ', ', ', ', ']|  [', ', ', ', ', ']|(262144,[186171],...|(262144,[186171],...|[8.45869192231039...|[0.65111331376428...|       0.0|\n",
      "|' Bored on aim wa...|     4.0|[', bored, on, ai...|[', bored, aim, w...|(262144,[9958,180...|(262144,[9958,180...|[6.87635157225856...|[0.07204831822632...|       4.0|\n",
      "|' Dancing with th...|     4.0|[', dancing, with...|[', dancing, devi...|(262144,[12409,17...|(262144,[12409,17...|[7.69664511705269...|[0.28723290968512...|       4.0|\n",
      "|' HEY YOU It's So...|     4.0|[', hey, you, it'...|[', hey, somaya, ...|(262144,[1303,510...|(262144,[1303,510...|[7.57311763817906...|[0.23875797885419...|       4.0|\n",
      "|' Happy Mother's ...|     4.0|[', happy, mother...|[', happy, mother...|(262144,[32629,14...|(262144,[32629,14...|[7.10352437785623...|[0.11013706916356...|       4.0|\n",
      "|' I'm not so sure...|     4.0|[', i'm, not, so,...|[', sure, deep, s...|(262144,[25328,10...|(262144,[25328,10...|[7.74053394366608...|[0.30725919597929...|       4.0|\n",
      "| ' LAW OF ATTRACTION|     4.0|[', law, of, attr...|[', law, attraction]|(262144,[87595,18...|(262144,[87595,18...|[7.78415551128375...|[0.32701090425684...|       4.0|\n",
      "|' Maybe you can p...|     4.0|[', maybe, you, c...|[', maybe, pop, t...|(262144,[1512,257...|(262144,[1512,257...|[7.24340665538033...|[0.14057447953801...|       4.0|\n",
      "|' Mothers others ...|     4.0|[', mothers, othe...|[', mothers, othe...|(262144,[13781,58...|(262144,[13781,58...|[5.07356774004527...|[0.00210692551050...|       4.0|\n",
      "|' Sidekicks' will...|     4.0|[', sidekicks', w...|[', sidekicks', a...|(262144,[106517,1...|(262144,[106517,1...|[7.90345269538588...|[0.38147732890734...|       4.0|\n",
      "|' They're easier ...|     4.0|[', they're, easi...|[', easier, witho...|(262144,[8562,180...|(262144,[8562,180...|[8.62617321208763...|[0.72318622507946...|       0.0|\n",
      "|' We are perfectl...|     4.0|[', we, are, perf...|[', perfectly, sc...|(262144,[9329,175...|(262144,[9329,175...|[7.28765450371981...|[0.15092665932032...|       4.0|\n",
      "|' We'll miss ye f...|     4.0|[', we'll, miss, ...|[', miss, ye, sum...|(262144,[1512,680...|(262144,[1512,680...|[8.94088608924159...|[0.82820596084071...|       0.0|\n",
      "|' alright lemme r...|     4.0|[', alright, lemm...|[', alright, lemm...|(262144,[53570,69...|(262144,[53570,69...|[7.72167366385461...|[0.29787625935054...|       4.0|\n",
      "|' but they're the...|     4.0|[', but, they're,...|[', men, take, ba...|(262144,[14273,55...|(262144,[14273,55...|[7.53936829736493...|[0.22783490683310...|       4.0|\n",
      "|                ' cc|     4.0|             [', cc]|             [', cc]|(262144,[72799,18...|(262144,[72799,18...|[8.09912715879675...|[0.47739350308525...|       4.0|\n",
      "|      ' give or take|     4.0| [', give, or, take]|     [', give, take]|(262144,[55639,10...|(262144,[55639,10...|[7.92898651998428...|[0.39349524267048...|       4.0|\n",
      "| ' have fun jadeyyyy|     4.0|[', have, fun, ja...|  [', fun, jadeyyyy]|(262144,[23087,87...|(262144,[23087,87...|[7.77794750316999...|[0.32457914540774...|       4.0|\n",
      "|' is just perfect...|     4.0|[', is, just, per...|[', perfect, idea...|(262144,[114591,1...|(262144,[114591,1...|[7.56512970370517...|[0.23797106481216...|       4.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|'We'll go back th...|     4.0|['we'll, go, back...|['we'll, go, back...|(262144,[37101,51...|(262144,[37101,51...|[7.43896857989907...|[0.19458961132409...|       4.0|\n",
      "|'s going to take ...|     4.0|['s, going, to, t...|['s, going, take,...|(262144,[34343,55...|(262144,[34343,55...|[9.39922736164059...|[0.92313038771066...|       0.0|\n",
      "|A Trainifique mom...|     4.0|[a, trainifique, ...|[trainifique, mom...|(262144,[1689,709...|(262144,[1689,709...|[6.15021911210636...|[0.01798250249653...|       4.0|\n",
      "|A aidan is a litt...|     4.0|[a, aidan, is, a,...|[aidan, little, c...|(262144,[102065,1...|(262144,[102065,1...|[7.44489095447696...|[0.19766141703238...|       4.0|\n",
      "|A bit tired Thick...|     4.0|[a, bit, tired, t...|[bit, tired, thic...|(262144,[31732,67...|(262144,[31732,67...|[8.02216301640648...|[0.43394056352886...|       4.0|\n",
      "|A joy ride from t...|     4.0|[a, joy, ride, fr...|[joy, ride, sligh...|(262144,[100314,1...|(262144,[100314,1...|[7.64580708258884...|[0.26705230025680...|       4.0|\n",
      "|A make shift neti...|     4.0|[a, make, shift, ...|[make, shift, net...|(262144,[89717,11...|(262144,[89717,11...|[9.18472045464182...|[0.88833356397646...|       0.0|\n",
      "|A plane is not wh...|     4.0|[a, plane, is, no...|[plane, want, avo...|(262144,[56998,71...|(262144,[56998,71...|[9.09714752585132...|[0.86931837716160...|       0.0|\n",
      "|A sad Feeling a l...|     4.0|[a, sad, feeling,...|[sad, feeling, li...|(262144,[9886,113...|(262144,[9886,113...|[10.4341621566106...|[0.98976631958160...|       0.0|\n",
      "|A thank you We ha...|     4.0|[a, thank, you, w...|[thank, blast, mo...|(262144,[39504,59...|(262144,[39504,59...|[5.18948209439870...|[0.00267004504269...|       4.0|\n",
      "|A will be ok Thin...|     4.0|[a, will, be, ok,...|[ok, think, jon, ...|(262144,[21823,75...|(262144,[21823,75...|[8.76813560280798...|[0.77322956627675...|       0.0|\n",
      "|A wins a win darling|     4.0|[a, wins, a, win,...|[wins, win, darling]|(262144,[56808,77...|(262144,[56808,77...|[7.42551011171706...|[0.19156725532254...|       4.0|\n",
      "|AAWWEESSOOMEEE i ...|     4.0|[aawweessoomeee, ...|[aawweessoomeee, ...|(262144,[7062,670...|(262144,[7062,670...|[7.80622354689188...|[0.33667532073725...|       4.0|\n",
      "|   ABS in hours time|     4.0|[abs, in, hours, ...|  [abs, hours, time]|(262144,[96266,12...|(262144,[96266,12...|[8.0013319725193,...|[0.42842746204390...|       4.0|\n",
      "|              AGREED|     4.0|            [agreed]|            [agreed]|(262144,[253205],...|(262144,[253205],...|[7.80447095074187...|[0.33655657161259...|       4.0|\n",
      "|        AGRREEEDDDDD|     4.0|      [agrreeeddddd]|      [agrreeeddddd]|(262144,[44354],[...|(262144,[44354],[...|[8.00400613317585...|[0.43101643877455...|       4.0|\n",
      "|AHHHH OMG I HAVE ...|     4.0|[ahhhh, omg, i, h...|[ahhhh, omg, get,...|(262144,[531,4755...|(262144,[531,4755...|[7.72305339960214...|[0.29760646821328...|       4.0|\n",
      "|AHHHHHH i remembe...|     4.0|[ahhhhhh, i, reme...|[ahhhhhh, remembe...|(262144,[91871,19...|(262144,[91871,19...|[7.36073452096596...|[0.17229564221112...|       4.0|\n",
      "|ALL us Blockheads...|     4.0|[all, us, blockhe...|[us, blockheads, ...|(262144,[33053,37...|(262144,[33053,37...|[7.67381214251064...|[0.27890086531255...|       4.0|\n",
      "|ALLi ThAt GiRL ha...|     4.0|[alli, that, girl...|[alli, girl, free...|(262144,[13781,16...|(262144,[13781,16...|[7.26621576169089...|[0.14506634104217...|       4.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|polarity|              words1|              words2|      term_frequency|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|''My mom got me d...|     4.0|[''my, mom, got, ...|[''my, mom, got, ...|(262144,[33123,35...|(262144,[33123,35...|[13.1874145613440...|[0.99995739186874...|       0.0|\n",
      "|'Nite girl Just s...|     4.0|['nite, girl, jus...|['nite, girl, sle...|(262144,[8421,137...|(262144,[8421,137...|[6.08812648896019...|[0.01573811462697...|       4.0|\n",
      "|'iPod for Dummies...|     4.0|['ipod, for, dumm...|['ipod, dummies',...|(262144,[39498,51...|(262144,[39498,51...|[7.69444861989296...|[0.28794969551716...|       4.0|\n",
      "|'s mum asked us t...|     4.0|['s, mum, asked, ...|['s, mum, asked, ...|(262144,[4790,817...|(262144,[4790,817...|[5.41045814987249...|[0.00414386369654...|       4.0|\n",
      "|'s music is the best|     4.0|['s, music, is, t...|   ['s, music, best]|(262144,[92492,13...|(262144,[92492,13...|[7.39243889792512...|[0.18157157955429...|       4.0|\n",
      "|                   A|     4.0|                 [a]|                  []|      (262144,[],[])|      (262144,[],[])|[8.00400613317585...|[0.43101643877455...|       4.0|\n",
      "|A That's alright ...|     4.0|[a, that's, alrig...|[alright, guess, ...|(262144,[24175,13...|(262144,[24175,13...|[7.85716108844026...|[0.35840524489097...|       4.0|\n",
      "|A bucket of ice w...|     4.0|[a, bucket, of, i...|[bucket, ice, goo...|(262144,[25581,41...|(262144,[25581,41...|[7.36531913974063...|[0.17309190022472...|       4.0|\n",
      "|A dog riding the ...|     4.0|[a, dog, riding, ...|[dog, riding, bic...|(262144,[32697,54...|(262144,[32697,54...|[7.48225098463358...|[0.20980594099555...|       4.0|\n",
      "|A few people aske...|     4.0|[a, few, people, ...|[people, asked, n...|(262144,[23087,26...|(262144,[23087,26...|[7.88485110823233...|[0.36889560670356...|       4.0|\n",
      "|A good teacher wi...|     4.0|[a, good, teacher...|[good, teacher, s...|(262144,[26819,52...|(262144,[26819,52...|[7.71449257049793...|[0.29406557214159...|       4.0|\n",
      "|A great start to ...|     4.0|[a, great, start,...|[great, start, gr...|(262144,[16004,65...|(262144,[16004,65...|[7.60604523667087...|[0.25188790554634...|       4.0|\n",
      "|A man can sing th...|     4.0|[a, man, can, sin...|[man, sing, love,...|(262144,[109753,1...|(262144,[109753,1...|[7.61762229118996...|[0.25787778912109...|       4.0|\n",
      "|A must watch Go L...|     4.0|[a, must, watch, ...|[must, watch, go,...|(262144,[148675,2...|(262144,[148675,2...|[7.69744334951721...|[0.28952404121671...|       4.0|\n",
      "|A storm is moving...|     4.0|[a, storm, is, mo...|[storm, moving, y...|(262144,[16989,57...|(262144,[16989,57...|[9.5648090601778,...|[0.94389993913789...|       0.0|\n",
      "|A thanks You're a...|     4.0|[a, thanks, you'r...|[thanks, always, ...|(262144,[64358,79...|(262144,[64358,79...|[6.58658918229729...|[0.04234783772916...|       4.0|\n",
      "|A this account is...|     4.0|[a, this, account...|[account, better,...|(262144,[25363,16...|(262144,[25363,16...|[7.76533721245481...|[0.31785482916122...|       4.0|\n",
      "|A thong that is f...|     4.0|[a, thong, that, ...|      [thong, funny]|(262144,[194194,2...|(262144,[194194,2...|[7.46724812415898...|[0.20517038384730...|       4.0|\n",
      "|A tweet up is whe...|     4.0|[a, tweet, up, is...|[tweet, tweeps, m...|(262144,[66669,10...|(262144,[66669,10...|[7.09675579995507...|[0.10890473564676...|       4.0|\n",
      "|A whole page of t...|     4.0|[a, whole, page, ...|[whole, page, twi...|(262144,[1512,762...|(262144,[1512,762...|[6.91952156211137...|[0.07858267873397...|       4.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trained_df = semantic_analysis_model.transform(training_data)\n",
    "val_df = semantic_analysis_model.transform(validation_data)\n",
    "test_df = semantic_analysis_model.transform(test_data)\n",
    "\n",
    "trained_df.show()\n",
    "val_df.show()\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data:\n",
      "Accuracy: 77.20708%\n",
      "Testing Data:\n",
      "Accuracy: 76.91339%\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"polarity\", metricName=\"accuracy\")\n",
    "accuracy_val = evaluator.evaluate(val_df)\n",
    "accuracy_test = evaluator.evaluate(test_df)\n",
    "print(\"Validation Data:\")\n",
    "print(f\"Accuracy: {accuracy_val*100:.5f}%\")\n",
    "print(\"Testing Data:\")\n",
    "print(f\"Accuracy: {accuracy_test*100:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.23780%\n"
     ]
    }
   ],
   "source": [
    "final_model = semantic_analysis_pipeline.fit(data)\n",
    "accuracy_test = evaluator.evaluate(final_model.transform(test_data))\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_test*100:.5f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(OUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
