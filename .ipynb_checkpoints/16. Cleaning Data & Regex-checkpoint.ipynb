{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression Resources:\n",
    "\n",
    "1) https://www.regular-expressions.info/tools.html\n",
    "\n",
    "\n",
    "2) https://www.freeformatter.com/java-regex-tester.html\n",
    "\n",
    "\n",
    "3) https://www.freeformatter.com\n",
    "\n",
    "\n",
    "# What we find while exploring the data in Notebook #15\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        We need to apply a proper schema\n",
    "    </li>\n",
    "    <li>\n",
    "    The date column need fixing\n",
    "    </li>\n",
    "    <li>\n",
    "    We need to extract/remove usernames\n",
    "    </li>\n",
    "    <li>\n",
    "    We need to extract hashtags and replace them with equivalent word\n",
    "    </li>\n",
    "    <li>\n",
    "    We need to remove URLs as our algorithm will not understand them\n",
    "    </li>\n",
    "    <li>\n",
    "    The same goes for email addresses\n",
    "    </li>\n",
    "    <li>\n",
    "    Symbols stored in HTML notation do not appear properly unescaped(example: &lt;)\n",
    "    </li>\n",
    "    <li>\n",
    "    Unwanted characters are present like starts or black dotted shapes\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "        .appName(\"data-cleaning\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "#Show unlimited Columns\n",
    "pd.options.display.max_columns = None\n",
    "#Show max 250 rows \n",
    "pd.options.display.max_rows = 250\n",
    "#Max col width = 150 as max tweet size is 144\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "\n",
    "\n",
    "# The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "# 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "# 1 - the id of the tweet (2087)\n",
    "# 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "# 3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "# 4 - the user that tweeted (robotickilldozr)\n",
    "# 5 - the text of the tweet (Lyx is cool)\n",
    "schema = \"polarity FLOAT, id LONG, date_time TIMESTAMP,query STRING,user STRING,text STRING\"\n",
    "timestampformat = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "\n",
    "#patH for INPUT & OUTPUT\n",
    "IN_PATH = \"datasets/sentiment-140-training-data/RAW\"\n",
    "OUT_PATH = \"datasets/sentiment-140-training-data/CLEAN\"\n",
    "\n",
    "#Declaring a common schema\n",
    "spark_reader = spark.read.schema(schema)\n",
    "\n",
    "@f.udf\n",
    "def html_unescape(s: str):\n",
    "    if isinstance(s, str):\n",
    "        return html.unescape(s)\n",
    "    return s\n",
    "\n",
    "#REGEX\n",
    "user_regex = r\"(@\\w{1,15})\"\n",
    "hashtag_regex = r\"(#\\w{1,})\"\n",
    "url_regex = r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n",
    "email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"original_text\",f.col(\"text\"))\n",
    "        .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),url_regex,\"\"))\n",
    "        .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),email_regex,\"\"))\n",
    "        .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),user_regex,\"\"))\n",
    "        .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),\"#\",\" \"))\n",
    "        .withColumn(\"text\",html_unescape(f.col(\"text\")))\n",
    "        .filter(\"text != ''\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_raw = spark_reader.csv(IN_PATH,timestampFormat=timestampformat)\n",
    "df_clean = clean_data(df_raw)\n",
    "\n",
    "df_clean.write.partitionBy(\"polarity\").parquet(OUT_PATH , mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1597303"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
