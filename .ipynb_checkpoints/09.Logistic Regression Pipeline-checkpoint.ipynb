{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer,VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pipelines\").getOrCreate()\n",
    "\n",
    "PATH = \"C:\\\\Spark\\\\data\\\\mllib\\\\sample_libsvm_data.txt\"\n",
    "\n",
    "#Load and parse the data file, converting it to a DataFrame\n",
    "\n",
    "sample_libsvm_data = (\n",
    "    spark.read.format(\"libsvm\")\n",
    "    .load(PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Stages:\n",
    "\n",
    "#Stage 1:\n",
    "# Index labels,adding metadata to the label column\n",
    "# Fit on whole dataset to include all labels in index\n",
    "label_indexer = StringIndexer(inputCol=\"label\",outputCol=\"indexedLabel\").fit(sample_libsvm_data)\n",
    "\n",
    "#Stage 2\n",
    "# Automatically identify categorial features, and index them\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuos\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\",outputCol=\"indexedFeatures\",maxCategories=4,\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "#Stage 3\n",
    "# Train a DecisionTree model.\n",
    "decision_tree_classifier_model = DecisionTreeClassifier(\n",
    "    labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Pipeline\n",
    "\n",
    "#Chain indexers and tree in a pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        label_indexer,  #STAGE 1\n",
    "        feature_indexer,  #STAGE 2\n",
    "        decision_tree_classifier_model #STAGE 3\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[98,99,100,1...|\n",
      "|       1.0|         1.0|(692,[122,123,124...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error= 0.038462\n"
     ]
    }
   ],
   "source": [
    "#Split into train-test\n",
    "(training_data,test_data) = sample_libsvm_data.randomSplit([0.7,0.3])\n",
    "\n",
    "#Train model. This also runs the indexer\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "#Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "#Select example rows and display\n",
    "predictions.select(\"prediction\",\"indexedLabel\",\"features\").show(5)\n",
    "\n",
    "#Select (prediction,true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\",metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error= {1.0-accuracy:5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexerModel: uid=StringIndexer_e755303f0811, handleInvalid=error, VectorIndexerModel: uid=VectorIndexer_cd49c575c440, numFeatures=692, handleInvalid=error, DecisionTreeClassifier_28814d985c43]\n",
      "Pipeline_837b66878d34__stages\n"
     ]
    }
   ],
   "source": [
    "#You can see that the pipeline and pipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(pipeline.stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "sample_libsvm_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Stages:\n",
    "\n",
    "#Stage 1\n",
    "# Automatically identify categorial features, and index them\n",
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"indexedFeatures\",\n",
    "    maxCategories=4,\n",
    ").fit(sample_libsvm_data)\n",
    "\n",
    "#Stage 2\n",
    "# Train a RandomForest model.\n",
    "random_forest_model = RandomForestRegressor(\n",
    "    featuresCol=\"indexedFeatures\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages =  [\n",
    "        feature_indexer,\n",
    "        random_forest_model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[98,99,100,1...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "RMSE: 0.190629\n"
     ]
    }
   ],
   "source": [
    "#Split the data into train-test sets\n",
    "(train_data,test_data) = sample_libsvm_data.randomSplit([0.7,0.3])\n",
    "\n",
    "#Train the model(pipeline)\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "#Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "predictions.select(\"prediction\",\"label\",\"features\").show(5)\n",
    "\n",
    "#Compute error by actual & predicted\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"RMSE: %g\"%rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VectorIndexerModel: uid=VectorIndexer_f5b261a8595a, numFeatures=692, handleInvalid=error, RandomForestRegressor_292ff18f1976]\n",
      "Pipeline_2e4e2c3e02ff__stages\n"
     ]
    }
   ],
   "source": [
    "#You can see that the pipeline and pipelineModel have the same stages\n",
    "print(pipeline.getStages())\n",
    "print(pipeline.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel: uid=RandomForestRegressor_292ff18f1976, numTrees=20, numFeatures=692\n"
     ]
    }
   ],
   "source": [
    "#To see the last stage\n",
    "print(model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "Model: <class 'pyspark.ml.pipeline.PipelineModel'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pipeline: {type(pipeline)}\")\n",
    "print(f\"Model: {type(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/Loading the model(Persistence)\n",
    "\n",
    "<center><h3>Saving the model</h3></center>\n",
    "        <center>Pipeline.write.save(PATH)<br>\n",
    "        or in short,<br>\n",
    "        Pipeline.save(PATH)</center>\n",
    "        \n",
    "<center><h3>Loading the model</h3></center>\n",
    "        <center>Pipeline.read.load(PATH)<br>\n",
    "        or in short,<br>\n",
    "        Pipeline.load(PATH)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
